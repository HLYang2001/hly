- title: "Unifying Attention Heads and Task Vectors via Hidden State Geometry in In-Context Learning"
  authors:
    - name: yz
    - name: fy
    - name: hy
    - name: sh
  venue: recall
  year: 2024
  basic_url: "https://www.cambridge.org/core/journals/recall/article/does-checkingin-help-understanding-l2-learners-autonomous-checkin-behavior-in-an-englishlanguage-mooc-through-learning-analytics/B59DF584526CC512E33F4B0FEF7CDD42"
  pages: 45
  urls:
    - url: "https://www.cambridge.org/core/services/aop-cambridge-core/content/view/B59DF584526CC512E33F4B0FEF7CDD42/S0958344024000144a.pdf/does-checking-in-help-understanding-l2-learners-autonomous-check-in-behavior-in-an-english-language-mooc-through-learning-analytics.pdf"
      label: "PDF"
 
  abstract: "The unusual properties of in-context learning (ICL) have prompted investigations into the internal mechanisms of large language models. Prior work typically focuses on either special attention heads or task vectors at specific layers, but lacks a unified framework linking these components to the evolution of hidden states across layers that ultimately produce the model's output. In this paper, we propose such a framework for ICL in classification tasks by analyzing two geometric factors that govern performance: the separability and alignment of query hidden states. A fine-grained analysis of layer-wise dynamics reveals a striking two-stage mechanism: separability emerges in early layers, while alignment develops in later layers. Ablation studies further show that Previous Token Heads drive separability, while Induction Heads and task vectors enhance alignment. Our findings thus bridge the gap between attention heads and task vectors, offering a unified account of ICL's underlying mechanisms."
  id: "journal1"
  bibtex: |
    @article{zhang2024does,<br>
    &nbsp;&nbsp;&nbsp;&nbsp;title={Does checking-in help? Understanding L2 learnersâ€™ autonomous check-in behavior in an English-language MOOC through learning analytics},<br>
    &nbsp;&nbsp;&nbsp;&nbsp;author={Zhang, Yining and Yang, Fang and Yang, Haolin and Han, Shuyuan},<br>
    &nbsp;&nbsp;&nbsp;&nbsp;journal={ReCALL},<br>
    &nbsp;&nbsp;&nbsp;&nbsp;volume={36},<br>
    &nbsp;&nbsp;&nbsp;&nbsp;number={3},<br>
    &nbsp;&nbsp;&nbsp;&nbsp;pages={343--358},<br>
    &nbsp;&nbsp;&nbsp;&nbsp;year={2024},<br>
    &nbsp;&nbsp;&nbsp;&nbsp;publisher={Cambridge University Press}<br>
    }
