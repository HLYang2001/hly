- title: "Binary Autoencoder for Mechanistic Interpretability of Large Language Models"
  authors:
    - name: hc
    - name: hy
    - name: Brian M. Kurkoski
      url: "https://www.jaist.ac.jp/is/labs/bits/brian"
    - name: ni
  venue: ICLR
  year: 2026
  basic_url: "https://arxiv.org/abs/2509.20997"
  pages: 36
  urls:
    - url: "https://arxiv.org/pdf/2509.20997"
      label: "PDF"
    - url: "https://arxiv.org/abs/2509.20997"
      label: "arXiv"
  abstract: "Existing works are dedicated to untangling atomized numerical components (features) from the hidden states of Large Language Models (LLMs) for interpreting their mechanism. However, they typically rely on autoencoders constrained by some implicit training-time regularization on single training instances (i.e.,  normalization, top-k function, etc.), without an explicit guarantee of global sparsity among instances, causing a large amount of dense (simultaneously inactive) features, harming the feature sparsity and atomization. In this paper, we propose a novel autoencoder variant that enforces minimal entropy on minibatches of hidden activations, thereby promoting feature independence and sparsity across instances. For efficient entropy calculation, we discretize the hidden activations to 1-bit via a step function and apply gradient estimation to enable backpropagation, so that we term it as Binary Autoencoder (BAE) and empirically demonstrate two major applications: (1) Feature set entropy calculation. Entropy can be reliably estimated on binary hidden activations, which we empirically evaluate and leverage to characterize the inference dynamics of LLMs and In-context Learning. (2) Feature untangling. Similar to typical methods, BAE can extract atomized features from LLM's hidden states. To robustly evaluate such feature extraction capability, we refine traditional feature-interpretation methods to avoid unreliable handling of numerical tokens, and show that BAE avoids dense features while producing the largest number of interpretable ones among baselines, which confirms the effectiveness of BAE serving as a feature extractor."
  id: "arxiv2"
  bibtex: |
    @article{cho2025binary,<br>
      &nbsp;&nbsp;&nbsp;&nbsp;title={Binary Autoencoder for Mechanistic Interpretability of Large Language Models},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;author={Cho, Hakaze and Yang, Haolin and Kurkoski, Brian M. and Inoue, Naoya},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;journal={arXiv preprint arXiv:2509.20997},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;year={2025}<br>
    }

- title: "Mechanism of Task-oriented Information Removal in In-context Learning"
  authors:
    - name: hc
    - name: hy
    - name: Gouki Minegishi
      url: "https://gouki-minegishi.github.io/"
    - name: ni
  venue: ICLR
  year: 2026
  basic_url: "https://arxiv.org/abs/2509.21012"
  pages: 67
  urls:
    - url: "https://arxiv.org/pdf/2509.21012"
      label: "PDF"
    - url: "https://arxiv.org/abs/2509.21012"
      label: "arXiv"
  abstract: "In-context Learning (ICL) is an emerging few-shot learning paradigm based on modern Language Models (LMs), yet its inner mechanism remains unclear. In this paper, we investigate the mechanism through a novel perspective of information removal. Specifically, we demonstrate that in the zero-shot scenario, LMs encode queries into non-selective representations in hidden states containing information for all possible tasks, leading to arbitrary outputs without focusing on the intended task, resulting in near-zero accuracy. Meanwhile, we find that selectively removing specific information from hidden states by a low-rank filter effectively steers LMs toward the intended task. Building on these findings, by measuring the hidden states on carefully designed metrics, we observe that few-shot ICL effectively simulates such task-oriented information removal processes, selectively removing the redundant information from entangled non-selective representations, and improving the output based on the demonstrations, which constitutes a key mechanism underlying ICL. Moreover, we identify essential attention heads inducing the removal operation, termed Denoising Heads, which enables the ablation experiments blocking the information removal operation from the inference, where the ICL accuracy significantly degrades, especially when the correct label is absent from the few-shot demonstrations, confirming both the critical role of the information removal mechanism and denoising heads."
  id: "arxiv1"
  bibtex: |
    @article{cho2025mechanism,<br>
      &nbsp;&nbsp;&nbsp;&nbsp;title={Mechanism of Task-oriented Information Removal in In-context Learning},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;author={Cho, Hakaze and Yang, Haolin and Minegishi, Gouki and Inoue, Naoya},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;journal={arXiv preprint arXiv:2509.21012},<br>
      &nbsp;&nbsp;&nbsp;&nbsp;year={2025}<br>
    }
