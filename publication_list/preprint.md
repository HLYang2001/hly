1. [**Mechanistic Fine-tuning for In-context Learning**](https://arxiv.org/abs/2505.14233){:target="_blank"}  
   **Hakaze Cho**, Peng Luo, Mariko Kato, Rin Kaenbyou, Naoya Inoue
   Pre-print. 2025. 28 pages.  
   [\[PDF\]](https://arxiv.org/pdf/2505.14233){:target="_blank"} 
   [\[ArXiv\]](https://arxiv.org/abs/2505.14233){:target="_blank"} 
2. [**Measuring Intrinsic Dimension of Token Embeddings**](https://arxiv.org/abs/2503.02142){:target="_blank"}  
   Takuya Kataiwa, **Hakaze Cho**, Tetsushi Ohki  
   Pre-print. 2025. 6 pages.  
   [\[PDF\]](https://arxiv.org/pdf/2503.02142){:target="_blank"} 
   [\[ArXiv\]](https://arxiv.org/abs/2503.02142){:target="_blank"} 
3. [**Affinity and Diversity: A Unified Metric for Demonstration Selection via Internal Representations**](https://arxiv.org/abs/2502.14380){:target="_blank"}  
   Mariko Kato, **Hakaze Cho**, Yoshihiro Sakai, Naoya Inoue  
   Pre-print. 2025. 8 pages.  
   [\[PDF\]](https://arxiv.org/pdf/2502.14380){:target="_blank"} 
   [\[ArXiv\]](https://arxiv.org/abs/2502.14380){:target="_blank"} 
4. [**StaICC: Standardized Evaluation for Classification Task in In-context Learning**](https://arxiv.org/abs/2501.15708){:target="_blank"}  
   **Hakaze Cho**, Naoya Inoue.  
   Pre-print. 2025. 20 pages.  
   [\[PDF\]](https://arxiv.org/pdf/2501.15708){:target="_blank"} 
   [\[ArXiv\]](https://arxiv.org/abs/2501.15708){:target="_blank"} 
   [\[Github\]](https://github.com/hc495/StaICC){:target="_blank"} 
   [\[Package\]](https://pypi.org/project/StaICC/){:target="_blank"}
5. [**NoisyICL: A Little Noise in Model Parameters Calibrates In-context Learning**](https://arxiv.org/abs/2402.05515){:target="_blank"}  
   **Yufeng Zhao**, Yoshihiro Sakai, Naoya Inoue.  
   Pre-print. 2024. 20 pages.  
    [\[PDF\]](https://arxiv.org/pdf/2402.05515){:target="_blank"} 
    [\[ArXiv\]](https://arxiv.org/abs/2402.05515){:target="_blank"} 
    [\[Github\]](https://github.com/hc495/NoisyICL){:target="_blank"}
6. [**SkIn: Skimming-Intensive Long-Text Classification Using BERT for Medical Corpus**](https://arxiv.org/abs/2209.05741){:target="_blank"}  
   **Yufeng Zhao**, et al.  
   Pre-print. 2022. 14 pages.  
   [\[PDF\]](https://arxiv.org/pdf/2209.05741){:target="_blank"} 
   [\[ArXiv\]](https://arxiv.org/abs/2209.05741){:target="_blank"} 